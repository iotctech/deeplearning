<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>第 4 章 VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION | 深度学习文档集</title>
  <meta name="description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="第 4 章 VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION | 深度学习文档集" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  <meta name="github-repo" content="iotctech/deeplearning" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="第 4 章 VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION | 深度学习文档集" />
  
  <meta name="twitter:description" content="This is a minimal example of using the bookdown package to write a book. The output format for this example is bookdown::gitbook." />
  

<meta name="author" content="贝塔" />


<meta name="date" content="2020-04-04" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="经典深度学习网络模型.html"/>

<script src="libs/header-attrs-2.1/header-attrs.js"></script>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />












<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Deep Learning Literature</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> 写在前面</a></li>
<li class="chapter" data-level="2" data-path="Alexnet.html"><a href="Alexnet.html"><i class="fa fa-check"></i><b>2</b> ImageNet Classification with Deep Convolutional Neural Networks</a>
<ul>
<li class="chapter" data-level="2.1" data-path="Alexnet.html"><a href="Alexnet.html#abstract摘要"><i class="fa fa-check"></i><b>2.1</b> Abstract(摘要)</a></li>
<li class="chapter" data-level="2.2" data-path="Alexnet.html"><a href="Alexnet.html#introduction引言"><i class="fa fa-check"></i><b>2.2</b> Introduction(引言)</a></li>
<li class="chapter" data-level="2.3" data-path="Alexnet.html"><a href="Alexnet.html#the-dataset数据集"><i class="fa fa-check"></i><b>2.3</b> The Dataset（数据集）</a></li>
<li class="chapter" data-level="2.4" data-path="Alexnet.html"><a href="Alexnet.html#the-architecture架构"><i class="fa fa-check"></i><b>2.4</b> The Architecture（架构）</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="Alexnet.html"><a href="Alexnet.html#relu-nonlinearityrelu非线性"><i class="fa fa-check"></i><b>2.4.1</b> ReLU Nonlinearity（ReLU非线性）</a></li>
<li class="chapter" data-level="2.4.2" data-path="Alexnet.html"><a href="Alexnet.html#training-on-multiple-gpus多gpu训练"><i class="fa fa-check"></i><b>2.4.2</b> Training on Multiple GPUs（多GPU训练）</a></li>
<li class="chapter" data-level="2.4.3" data-path="Alexnet.html"><a href="Alexnet.html#local-response-normalization局部响应归一化"><i class="fa fa-check"></i><b>2.4.3</b> Local Response Normalization（局部响应归一化）</a></li>
<li class="chapter" data-level="2.4.4" data-path="Alexnet.html"><a href="Alexnet.html#overlapping-pooling重叠池化"><i class="fa fa-check"></i><b>2.4.4</b> Overlapping Pooling（重叠池化）</a></li>
<li class="chapter" data-level="2.4.5" data-path="Alexnet.html"><a href="Alexnet.html#overall-architecture整体架构"><i class="fa fa-check"></i><b>2.4.5</b> Overall Architecture（整体架构）</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="Alexnet.html"><a href="Alexnet.html#reducing-overfitting减小过拟合"><i class="fa fa-check"></i><b>2.5</b> Reducing Overfitting（减小过拟合）</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="Alexnet.html"><a href="Alexnet.html#data-augmentation数据增强"><i class="fa fa-check"></i><b>2.5.1</b> Data Augmentation（数据增强）</a></li>
<li class="chapter" data-level="2.5.2" data-path="Alexnet.html"><a href="Alexnet.html#dropout"><i class="fa fa-check"></i><b>2.5.2</b> Dropout</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="Alexnet.html"><a href="Alexnet.html#details-of-learning学习细节"><i class="fa fa-check"></i><b>2.6</b> Details of learning（学习细节）</a></li>
<li class="chapter" data-level="2.7" data-path="Alexnet.html"><a href="Alexnet.html#results结果"><i class="fa fa-check"></i><b>2.7</b> Results（结果）</a>
<ul>
<li class="chapter" data-level="2.7.1" data-path="Alexnet.html"><a href="Alexnet.html#qualitative-evaluations定性评估"><i class="fa fa-check"></i><b>2.7.1</b> Qualitative Evaluations（定性评估）</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="Alexnet.html"><a href="Alexnet.html#discussion探讨"><i class="fa fa-check"></i><b>2.8</b> Discussion（探讨）</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html"><i class="fa fa-check"></i><b>3</b> 经典深度学习网络模型</a>
<ul>
<li class="chapter" data-level="3.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#lenet-5"><i class="fa fa-check"></i><b>3.1</b> LeNet-5</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型介绍"><i class="fa fa-check"></i><b>3.1.1</b> 模型介绍</a></li>
<li class="chapter" data-level="3.1.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型结构"><i class="fa fa-check"></i><b>3.1.2</b> 模型结构</a></li>
<li class="chapter" data-level="3.1.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型特性"><i class="fa fa-check"></i><b>3.1.3</b> 模型特性</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#alexnet"><i class="fa fa-check"></i><b>3.2</b> AlexNet</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型介绍-1"><i class="fa fa-check"></i><b>3.2.1</b> 模型介绍</a></li>
<li class="chapter" data-level="3.2.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型结构-1"><i class="fa fa-check"></i><b>3.2.2</b> 模型结构</a></li>
<li class="chapter" data-level="3.2.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型特性-1"><i class="fa fa-check"></i><b>3.2.3</b> 模型特性</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#zfnet"><i class="fa fa-check"></i><b>3.3</b> ZFNet</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型介绍-2"><i class="fa fa-check"></i><b>3.3.1</b> 模型介绍</a></li>
<li class="chapter" data-level="3.3.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型结构-2"><i class="fa fa-check"></i><b>3.3.2</b> 模型结构</a></li>
<li class="chapter" data-level="3.3.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型特性-2"><i class="fa fa-check"></i><b>3.3.3</b> 模型特性</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#network-in-network"><i class="fa fa-check"></i><b>3.4</b> Network in Network</a>
<ul>
<li class="chapter" data-level="3.4.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型介绍-3"><i class="fa fa-check"></i><b>3.4.1</b> 模型介绍</a></li>
<li class="chapter" data-level="3.4.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型结构-3"><i class="fa fa-check"></i><b>3.4.2</b> 模型结构</a></li>
<li class="chapter" data-level="3.4.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型特点"><i class="fa fa-check"></i><b>3.4.3</b> 模型特点</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#vggnet"><i class="fa fa-check"></i><b>3.5</b> VGGNet</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型介绍-4"><i class="fa fa-check"></i><b>3.5.1</b> 模型介绍</a></li>
<li class="chapter" data-level="3.5.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型结构-4"><i class="fa fa-check"></i><b>3.5.2</b> 模型结构</a></li>
<li class="chapter" data-level="3.5.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型特性-3"><i class="fa fa-check"></i><b>3.5.3</b> 模型特性</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#googlenet"><i class="fa fa-check"></i><b>3.6</b> GoogLeNet</a>
<ul>
<li class="chapter" data-level="3.6.1" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型介绍-5"><i class="fa fa-check"></i><b>3.6.1</b> 模型介绍</a></li>
<li class="chapter" data-level="3.6.2" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型结构-5"><i class="fa fa-check"></i><b>3.6.2</b> 4.6.2 模型结构</a></li>
<li class="chapter" data-level="3.6.3" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#模型特性-4"><i class="fa fa-check"></i><b>3.6.3</b> 模型特性</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="经典深度学习网络模型.html"><a href="经典深度学习网络模型.html#为什么现在的cnn模型都是在googlenetvggnet或者alexnet上调整的"><i class="fa fa-check"></i><b>3.7</b> 为什么现在的CNN模型都是在GoogleNet、VGGNet或者AlexNet上调整的？</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><i class="fa fa-check"></i><b>4</b> VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</a>
<ul>
<li class="chapter" data-level="4.1" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#abstract"><i class="fa fa-check"></i><b>4.1</b> ABSTRACT</a></li>
<li class="chapter" data-level="4.2" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#introduction"><i class="fa fa-check"></i><b>4.2</b> INTRODUCTION</a></li>
<li class="chapter" data-level="4.3" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#convnet-configurations"><i class="fa fa-check"></i><b>4.3</b> CONVNET CONFIGURATIONS</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#architecture"><i class="fa fa-check"></i><b>4.3.1</b> ARCHITECTURE</a></li>
<li class="chapter" data-level="4.3.2" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#configurations"><i class="fa fa-check"></i><b>4.3.2</b> CONFIGURATIONS</a></li>
<li class="chapter" data-level="4.3.3" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#discussion"><i class="fa fa-check"></i><b>4.3.3</b> DISCUSSION</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#classification-framework"><i class="fa fa-check"></i><b>4.4</b> CLASSIFICATION FRAMEWORK</a>
<ul>
<li class="chapter" data-level="4.4.1" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#training"><i class="fa fa-check"></i><b>4.4.1</b> TRAINING</a></li>
<li class="chapter" data-level="4.4.2" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#testing"><i class="fa fa-check"></i><b>4.4.2</b> TESTING</a></li>
<li class="chapter" data-level="4.4.3" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#implementation-details"><i class="fa fa-check"></i><b>4.4.3</b> IMPLEMENTATION DETAILS</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#classification-experiments"><i class="fa fa-check"></i><b>4.5</b> CLASSIFICATION EXPERIMENTS</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#single-scale-evaluation"><i class="fa fa-check"></i><b>4.5.1</b> SINGLE SCALE EVALUATION</a></li>
<li class="chapter" data-level="4.5.2" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#multi-scale-evaluation"><i class="fa fa-check"></i><b>4.5.2</b> MULTI-SCALE EVALUATION</a></li>
<li class="chapter" data-level="4.5.3" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#multi-crop-evaluation"><i class="fa fa-check"></i><b>4.5.3</b> MULTI-CROP EVALUATION</a></li>
<li class="chapter" data-level="4.5.4" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#convnet-fusion"><i class="fa fa-check"></i><b>4.5.4</b> CONVNET FUSION</a></li>
<li class="chapter" data-level="4.5.5" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#comparison-with-the-state-of-the-art"><i class="fa fa-check"></i><b>4.5.5</b> COMPARISON WITH THE STATE OF THE ART</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="very-deep-convolutional-networks-for-large-scale-image-recognition.html"><a href="very-deep-convolutional-networks-for-large-scale-image-recognition.html#conclusion"><i class="fa fa-check"></i><b>4.6</b> CONCLUSION</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">本论文由 bookdown 强力驱动</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">深度学习文档集</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="very-deep-convolutional-networks-for-large-scale-image-recognition" class="section level1" number="4">
<h1><span class="header-section-number">第 4 章</span> VERY DEEP CONVOLUTIONAL NETWORKS FOR LARGE-SCALE IMAGE RECOGNITION</h1>
<div id="abstract" class="section level2" number="4.1">
<h2><span class="header-section-number">4.1</span> ABSTRACT</h2>
<p>In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3 × 3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16–19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.</p>
</div>
<div id="introduction" class="section level2" number="4.2">
<h2><span class="header-section-number">4.2</span> INTRODUCTION</h2>
<p>Convolutional networks (ConvNets) have recently enjoyed a great success in large-scale image and video recognition (Krizhevsky et al., 2012; Zeiler &amp; Fergus, 2013; Sermanet et al., 2014; Simonyan &amp; Zisserman, 2014) which has become possible due to the large public image repositories, such as ImageNet (Deng et al., 2009), and high-performance computing systems, such as GPUs or large-scale distributed clusters (Dean et al., 2012). In particular, an important role in the advance of deep visual recognition architectures has been played by the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) (Russakovsky et al., 2014), which has served as a testbed for a few generations of large-scale image classification systems, from high-dimensional shallow feature encodings (Perronnin et al., 2010) (the winner of ILSVRC-2011) to deep ConvNets (Krizhevsky et al., 2012) (the winner of ILSVRC-2012).</p>
<p>With ConvNets becoming more of a commodity in the computer vision field, a number of attempts have been made to improve the original architecture of Krizhevsky et al. (2012) in a bid to achieve better accuracy. For instance, the best-performing submissions to the ILSVRC-2013 (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014) utilised smaller receptive window size and smaller stride of the first convolutional layer. Another line of improvements dealt with training and testing the networks densely over the whole image and over multiple scales (Sermanet et al., 2014; Howard, 2014). In this paper, we address another important aspect of ConvNet architecture design – its depth. To this end, we fix other parameters of the architecture, and steadily increase the depth of the network by adding more convolutional layers, which is feasible due to the use of very small (3×3) convolution filters in all layers.</p>
<p>As a result, we come up with significantly more accurate ConvNet architectures, which not only achieve the state-of-the-art accuracy on ILSVRC classification and localisation tasks, but are also applicable to other image recognition datasets, where they achieve excellent performance even when used as a part of a relatively simple pipelines (e.g. deep features classified by a linear SVM without fine-tuning). We have released our two best-performing models to facilitate further research.</p>
<p>The rest of the paper is organised as follows. In Sect. 2, we describe our ConvNet configurations. The details of the image classification training and evaluation are then presented in Sect. 3, and the configurations are compared on the ILSVRC classification task in Sect. 4. Sect. 5 concludes the paper. For completeness, we also describe and assess our ILSVRC-2014 object localisation system in Appendix A, and discuss the generalisation of very deep features to other datasets in Appendix B. Finally, Appendix C contains the list of major paper revisions.</p>
</div>
<div id="convnet-configurations" class="section level2" number="4.3">
<h2><span class="header-section-number">4.3</span> CONVNET CONFIGURATIONS</h2>
<p>To measure the improvement brought by the increased ConvNet depth in a fair setting, all our ConvNet layer configurations are designed using the same principles, inspired by Ciresan et al. (2011); Krizhevsky et al. (2012). In this section, we first describe a generic layout of our ConvNet configurations (Sect. 2.1) and then detail the specific configurations used in the evaluation (Sect. 2.2). Our design choices are then discussed and compared to the prior art in Sect. 2.3.</p>
<div id="architecture" class="section level3" number="4.3.1">
<h3><span class="header-section-number">4.3.1</span> ARCHITECTURE</h3>
<p>During training, the input to our ConvNets is a fixed-size 224 × 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel. The image is passed through a stack of convolutional (conv.) layers, where we use filters with a very small receptive field: 3 × 3 (which is the smallest size to capture the notion of left/right, up/down, center). In one of the configurations we also utilise 1 × 1 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1 pixel for 3 × 3 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2 × 2 pixel window, with stride 2.</p>
<p>A stack of convolutional layers (which has a different depth in different architectures) is followed by three Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.</p>
<p>All hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity. We note that none of our networks (except for one) contain Local Response Normalisation (LRN) normalisation (Krizhevsky et al., 2012): as will be shown in Sect. 4, such normalisation does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time. Where applicable, the parameters for the LRN layer are those of (Krizhevsky et al., 2012).</p>
</div>
<div id="configurations" class="section level3" number="4.3.2">
<h3><span class="header-section-number">4.3.2</span> CONFIGURATIONS</h3>
<p>The ConvNet configurations, evaluated in this paper, are outlined in Table 1, one per column. In the following we will refer to the nets by their names (A–E). All configurations follow the generic design presented in Sect. 2.1, and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.</p>
<p>In Table 2 we report the number of parameters for each configuration. In spite of a large depth, the number of weights in our nets is not greater than the number of weights in a more shallow net with larger conv. layer widths and receptive fields (144M weights in (Sermanet et al., 2014)).</p>
<p>Table 1: <strong>ConvNet configurations</strong> (shown in columns). The depth of the configurations increases from the left (A) to the right (E), as more layers are added (the added layers are shown in bold). The convolutional layer parameters are denoted as “<span class="math inline">\(conv \left\langle receptive field size \right\rangle - \left\langle number of channels \right\rangle\)</span>”. The ReLU activation function is not shown for brevity.</p>
<p><img src="img/03-01.png" width="70%" style="display: block; margin: auto;" /></p>
<p><img src="img/03-02.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="discussion" class="section level3" number="4.3.3">
<h3><span class="header-section-number">4.3.3</span> DISCUSSION</h3>
<p>Our ConvNet configurations are quite different from the ones used in the top-performing entries of the ILSVRC-2012 (Krizhevsky et al., 2012) and ILSVRC-2013 competitions (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014). Rather than using relatively large receptive fields in the first conv. layers (e.g. <span class="math inline">\(11×11\)</span> with stride 4 in (Krizhevsky et al., 2012), or <span class="math inline">\(7×7\)</span> with stride 2 in (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014)), we use very small <span class="math inline">\(3 × 3\)</span> receptive fields throughout the whole net, which are convolved with the input at every pixel (with stride 1). It is easy to see that a stack of two <span class="math inline">\(3×3\)</span> conv. layers (without spatial pooling in between) has an effective receptive field of 5×5; three such layers have a <span class="math inline">\(7 × 7\)</span> effective receptive field. So what have we gained by using, for instance, a stack of three 3×3 conv. layers instead of a single <span class="math inline">\(7×7\)</span> layer? First, we incorporate three non-linear rectification layers instead of a single one, which makes the decision function more discriminative. Second, we decrease the number of parameters: assuming that both the input and the output of a three-layer <span class="math inline">\(3 × 3\)</span> convolution stack has <span class="math inline">\(C\)</span> channels, the stack is parametrised by <span class="math inline">\(3(3^2C^2) = 27C^2\)</span> weights; at the same time, a single <span class="math inline">\(7 × 7\)</span> conv. layer would require <span class="math inline">\(7^2C^2 = 49C^2\)</span> parameters, i.e. <span class="math inline">\(81%\)</span> more. This can be seen as imposing a regularisation on the <span class="math inline">\(7 × 7\)</span> conv. filters, forcing them to have a decomposition through the <span class="math inline">\(3 × 3\)</span> filters (with non-linearity injected in between).</p>
<p>The incorporation of 1 × 1 conv. layers (configuration C, Table 1) is a way to increase the non-linearity of the decision function without affecting the receptive fields of the conv. layers. Even though in our case the 1 × 1 convolution is essentially a linear projection onto the space of the same dimensionality (the number of input and output channels is the same), an additional non-linearity is introduced by the rectification function. It should be noted that 1 × 1 conv. layers have recently been utilised in the “Network in Network” architecture of Lin et al. (2014).</p>
<p>Small-size convolution filters have been previously used by Ciresan et al. (2011), but their nets are significantly less deep than ours, and they did not evaluate on the large-scale ILSVRC dataset. Goodfellow et al. (2014) applied deep ConvNets (11 weight layers) to the task of street number recognition, and showed that the increased depth led to better performance. GoogLeNet (Szegedy et al., 2014), a top-performing entry of the ILSVRC-2014 classification task, was developed independently of our work, but is similar in that it is based on very deep ConvNets(22 weight layers) and small convolution filters (apart from 3 × 3, they also use 1 × 1 and 5 × 5 convolutions). Their network topology is, however, more complex than ours, and the spatial resolution of the feature maps is reduced more aggressively in the first layers to decrease the amount of computation. As will be shown in Sect. 4.5, our model is outperforming that of Szegedy et al. (2014) in terms of the single-network classification accuracy.</p>
</div>
</div>
<div id="classification-framework" class="section level2" number="4.4">
<h2><span class="header-section-number">4.4</span> CLASSIFICATION FRAMEWORK</h2>
<p>In the previous section we presented the details of our network configurations. In this section, we describe the details of classification ConvNet training and evaluation.</p>
<div id="training" class="section level3" number="4.4.1">
<h3><span class="header-section-number">4.4.1</span> TRAINING</h3>
<p>The ConvNet training procedure generally follows Krizhevsky et al. (2012) (except for sampling the input crops from multi-scale training images, as explained later). Namely, the training is carried out by optimising the multinomial logistic regression objective using mini-batch gradient descent (based on back-propagation (LeCun et al., 1989)) with momentum. The batch size was set to 256, momentum to 0.9. The training was regularised by weight decay (the <span class="math inline">\(L_2\)</span> penalty multiplier set to <span class="math inline">\(5 · 10^{−4}\)</span>) and dropout regularisation for the first two fully-connected layers (dropout ratio set to 0.5). The learning rate was initially set to <span class="math inline">\(10^{−2}\)</span>, and then decreased by a factor of 10 when the validation set accuracy stopped improving. In total, the learning rate was decreased 3 times, and the learning was stopped after 370K iterations (74 epochs). We conjecture that in spite of the larger number of parameters and the greater depth of our nets compared to (Krizhevsky et al., 2012), the nets required less epochs to converge due to (a) implicit regularisation imposed by greater depth and smaller conv. filter sizes; (b) pre-initialisation of certain layers.</p>
<p>The initialisation of the network weights is important, since bad initialisation can stall learning due to the instability of gradient in deep nets. To circumvent this problem, we began with training the configuration A (Table 1), shallow enough to be trained with random initialisation. Then, when training deeper architectures, we initialised the first four convolutional layers and the last three fully-connected layers with the layers of net A (the intermediate layers were initialised randomly). We did not decrease the learning rate for the pre-initialised layers, allowing them to change during learning. For random initialisation (where applicable), we sampled the weights from a normal distribution with the zero mean and <span class="math inline">\(10^−2\)</span> variance. The biases were initialised with zero. It is worth noting that after the paper submission we found that it is possible to initialise the weights without pre-training by using the random initialisation procedure of Glorot &amp; Bengio (2010).</p>
<p>To obtain the fixed-size 224×224 ConvNet input images, they were randomly cropped from rescaled training images (one crop per image per SGD iteration). To further augment the training set, the crops underwent random horizontal flipping and random RGB colour shift (Krizhevsky et al., 2012). Training image rescaling is explained below.</p>
<p><strong>Training image size.</strong> Let <span class="math inline">\(S\)</span> be the smallest side of an isotropically-rescaled training image, from which the ConvNet input is cropped (we also refer to <span class="math inline">\(S\)</span> as the training scale). While the crop size is fixed to 224 × 224, in principle S can take on any value not less than 224: for <span class="math inline">\(S\)</span> = 224 the crop will capture whole-image statistics, completely spanning the smallest side of a training image; for <span class="math inline">\(S\)</span> ≫ 224 the crop will correspond to a small part of the image, containing a small object or an object part.</p>
<p>We consider two approaches for setting the training scale <span class="math inline">\(S\)</span>. The first is to fix <span class="math inline">\(S\)</span>, which corresponds to single-scale training (note that image content within the sampled crops can still represent multi-scale image statistics). In our experiments, we evaluated models trained at two fixed scales: <span class="math inline">\(S\)</span> = 256 (which has been widely used in the prior art (Krizhevsky et al., 2012; Zeiler &amp; Fergus, 2013; Sermanet et al., 2014)) and <span class="math inline">\(S\)</span> = 384. Given a ConvNet configuration, we first trained the network using S = 256. To speed-up training of the <span class="math inline">\(S\)</span> = 384 network, it was initialised with the weights pre-trained with <span class="math inline">\(S\)</span> = 256, and we used a smaller initial learning rate of <span class="math inline">\(10^{-3}\)</span></p>
<p>The second approach to setting <span class="math inline">\(S\)</span> is multi-scale training, where each training image is individually rescaled by randomly sampling <span class="math inline">\(S\)</span> from a certain range [<span class="math inline">\(S_{min}\)</span>, <span class="math inline">\(S_{max}\)</span>] (we used <span class="math inline">\(S_{min}\)</span> = 256 and <span class="math inline">\(S_{max}\)</span> = 512). Since objects in images can be of different size, it is beneficial to take this into account during training. This can also be seen as training set augmentation by scale jittering, where a single model is trained to recognise objects over a wide range of scales. For speed reasons, we trained multi-scale models by fine-tuning all layers of a single-scale model with the same configuration, pre-trained with fixed <span class="math inline">\(S\)</span> = 384.</p>
</div>
<div id="testing" class="section level3" number="4.4.2">
<h3><span class="header-section-number">4.4.2</span> TESTING</h3>
<p>At test time, given a trained ConvNet and an input image, it is classified in the following way. First, it is isotropically rescaled to a pre-defined smallest image side, denoted as <span class="math inline">\(Q\)</span> (we also refer to it as the test scale). We note that <span class="math inline">\(Q\)</span> is not necessarily equal to the training scale <span class="math inline">\(S\)</span> (as we will show in Sect. 4, using several values of <span class="math inline">\(Q\)</span> for each <span class="math inline">\(S\)</span> leads to improved performance). Then, the network is applied densely over the rescaled test image in a way similar to (Sermanet et al., 2014). Namely, the fully-connected layers are first converted to convolutional layers (the first FC layer to a 7 × 7 conv. layer, the last two FC layers to 1 × 1 conv. layers). The resulting fully-convolutional net is then applied to the whole (uncropped) image. The result is a class score map with the number of channels equal to the number of classes, and a variable spatial resolution, dependent on the input image size. Finally, to obtain a fixed-size vector of class scores for the image, the class score map is spatially averaged (sum-pooled). We also augment the test set by horizontal flipping of the images; the soft-max class posteriors of the original and flipped images are averaged to obtain the final scores for the image.</p>
<p>Since the fully-convolutional network is applied over the whole image, there is no need to sample multiple crops at test time (Krizhevsky et al., 2012), which is less efficient as it requires network re-computation for each crop. At the same time, using a large set of crops, as done by Szegedy et al. (2014), can lead to improved accuracy, as it results in a finer sampling of the input image compared to the fully-convolutional net. Also, multi-crop evaluation is complementary to dense evaluation due to different convolution boundary conditions: when applying a ConvNet to a crop, the convolved feature maps are padded with zeros, while in the case of dense evaluation the padding for the same crop naturally comes from the neighbouring parts of an image (due to both the convolutions and spatial pooling), which substantially increases the overall network receptive field, so more context is captured. While we believe that in practice the increased computation time of multiple crops does not justify the potential gains in accuracy, for reference we also evaluate our networks using 50 crops per scale (5 × 5 regular grid with 2 flips), for a total of 150 crops over 3 scales, which is comparable to 144 crops over 4 scales used by Szegedy et al. (2014).</p>
</div>
<div id="implementation-details" class="section level3" number="4.4.3">
<h3><span class="header-section-number">4.4.3</span> IMPLEMENTATION DETAILS</h3>
<p>Our implementation is derived from the publicly available C++ Caffe toolbox (Jia, 2013) (branched out in December 2013), but contains a number of significant modifications, allowing us to perform training and evaluation on multiple GPUs installed in a single system, as well as train and evaluate on full-size (uncropped) images at multiple scales (as described above). Multi-GPU training exploits data parallelism, and is carried out by splitting each batch of training images into several GPU batches, processed in parallel on each GPU. After the GPU batch gradients are computed, they are averaged to obtain the gradient of the full batch. Gradient computation is synchronous across the GPUs, so the result is exactly the same as when training on a single GPU.</p>
<p>While more sophisticated methods of speeding up ConvNet training have been recently proposed (Krizhevsky, 2014), which employ model and data parallelism for different layers of the net, we have found that our conceptually much simpler scheme already provides a speedup of 3.75 times on an off-the-shelf 4-GPU system, as compared to using a single GPU. On a system equipped with four NVIDIA Titan Black GPUs, training a single net took 2–3 weeks depending on the architecture.</p>
</div>
</div>
<div id="classification-experiments" class="section level2" number="4.5">
<h2><span class="header-section-number">4.5</span> CLASSIFICATION EXPERIMENTS</h2>
<p><strong>Dataset.</strong> In this section, we present the image classification results achieved by the described ConvNet architectures on the ILSVRC-2012 dataset (which was used for ILSVRC 2012–2014 challenges). The dataset includes images of 1000 classes, and is split into three sets: training (1.3M images), validation (50K images), and testing (100K images with held-out class labels). The classification performance is evaluated using two measures: the top-1 and top-5 error. The former is a multi-class classification error, i.e. the proportion of incorrectly classified images; the latter is the main evaluation criterion used in ILSVRC, and is computed as the proportion of images such that the ground-truth category is outside the top-5 predicted categories.</p>
<p>For the majority of experiments, we used the validation set as the test set. Certain experiments were also carried out on the test set and submitted to the official ILSVRC server as a “VGG” team entry to the ILSVRC-2014 competition (Russakovsky et al., 2014).</p>
<div id="single-scale-evaluation" class="section level3" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> SINGLE SCALE EVALUATION</h3>
<p>We begin with evaluating the performance of individual ConvNet models at a single scale with the layer configurations described in Sect. 2.2. The test image size was set as follows: <span class="math inline">\(Q = S\)</span> for fixed <span class="math inline">\(S\)</span>, and <span class="math inline">\(Q\)</span> = 0.5(<span class="math inline">\(S_{min}\)</span> + <span class="math inline">\(S_{max}\)</span>) for jittered <span class="math inline">\(S \in [S_{min}, S_{max}]\)</span>. The results of are shown in Table 3.</p>
<p>First, we note that using local response normalisation (A-LRN network) does not improve on the model A without any normalisation layers. We thus do not employ normalisation in the deeper architectures (B–E).</p>
<p>Second, we observe that the classification error decreases with the increased ConvNet depth: from 11 layers in A to 19 layers in E. Notably, in spite of the same depth, the configuration C (which contains three 1 × 1 conv. layers), performs worse than the configuration D, which uses 3 × 3 conv. layers throughout the network. This indicates that while the additional non-linearity does help (C is better than B), it is also important to capture spatial context by using conv. filters with non-trivial receptive fields (D is better than C). The error rate of our architecture saturates when the depth reaches 19 layers, but even deeper models might be beneficial for larger datasets. We also compared the net B with a shallow net with five 5 × 5 conv. layers, which was derived from B by replacing each pair of 3 × 3 conv. layers with a single 5 × 5 conv. layer (which has the same receptive field as explained in Sect. 2.3). The top-1 error of the shallow net was measured to be 7% higher than that of B (on a center crop), which confirms that a deep net with small filters outperforms a shallow net with larger filters.</p>
<p>Finally, scale jittering at training time (<span class="math inline">\(S \in [256; 512]\)</span>) leads to significantly better results than training on images with fixed smallest side (<span class="math inline">\(S\)</span> = 256 or <span class="math inline">\(S\)</span> = 384), even though a single scale is used at test time. This confirms that training set augmentation by scale jittering is indeed helpful for capturing multi-scale image statistics.</p>
<p><img src="img/03-03.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="multi-scale-evaluation" class="section level3" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> MULTI-SCALE EVALUATION</h3>
<p>Having evaluated the ConvNet models at a single scale, we now assess the effect of scale jittering at test time. It consists of running a model over several rescaled versions of a test image (corresponding to different values of <span class="math inline">\(Q\)</span>), followed by averaging the resulting class posteriors. Considering that a large discrepancy between training and testing scales leads to a drop in performance, the models trained with fixed S were evaluated over three test image sizes, close to the training one: <span class="math inline">\(Q = {S − 32, S, S + 32}\)</span>. At the same time, scale jittering at training time allows the network to be applied to a wider range of scales at test time, so the model trained with variable <span class="math inline">\(S \in [S_{min}; S_{max}]\)</span> was evaluated over a larger range of sizes <span class="math inline">\(Q = {S_{min}, 0.5(S_{min} + S_{max}), S_{max}}\)</span>.</p>
<p>The results, presented in Table 4, indicate that scale jittering at test time leads to better performance (as compared to evaluating the same model at a single scale, shown in Table 3). As before, the deepest configurations (D and E) perform the best, and scale jittering is better than training with a fixed smallest side S. Our best single-network performance on the validation set is 24.8%/7.5% top-1/top-5 error (highlighted in bold in Table 4). On the test set, the configuration E achieves 7.3% top-5 error.</p>
<p><img src="img/03-04.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="multi-crop-evaluation" class="section level3" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> MULTI-CROP EVALUATION</h3>
<p>In Table 5 we compare dense ConvNet evaluation with mult-crop evaluation (see Sect. 3.2 for details). We also assess the complementarity of the two evaluation techniques by averaging their softmax outputs. As can be seen, using multiple crops performs slightly better than dense evaluation, and the two approaches are indeed complementary, as their combination outperforms each of them. As noted above, we hypothesize that this is due to a different treatment of convolution boundary conditions.</p>
<p>Table 5: <strong>ConvNet evaluation techniques comparison.</strong> In all experiments the training scale <span class="math inline">\(S\)</span> was sampled from [256; 512], and three test scales <span class="math inline">\(Q\)</span> were considered: {256, 384, 512}.</p>
<p><img src="img/03-05.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
<div id="convnet-fusion" class="section level3" number="4.5.4">
<h3><span class="header-section-number">4.5.4</span> CONVNET FUSION</h3>
<p>Up until now, we evaluated the performance of individual ConvNet models. In this part of the experiments, we combine the outputs of several models by averaging their soft-max class posteriors. This improves the performance due to complementarity of the models, and was used in the top ILSVRC submissions in 2012 (Krizhevsky et al., 2012) and 2013 (Zeiler &amp; Fergus, 2013; Sermanet et al., 2014).</p>
<p>The results are shown in Table 6. By the time of ILSVRC submission we had only trained the single-scale networks, as well as a multi-scale model D (by fine-tuning only the fully-connected layers rather than all layers). The resulting ensemble of 7 networks has 7.3% ILSVRC test error. After the submission, we considered an ensemble of only two best-performing multi-scale models (configurations D and E), which reduced the test error to 7.0% using dense evaluation and 6.8% using combined dense and multi-crop evaluation. For reference, our best-performing single model achieves 7.1% error (model E, Table 5).</p>
</div>
<div id="comparison-with-the-state-of-the-art" class="section level3" number="4.5.5">
<h3><span class="header-section-number">4.5.5</span> COMPARISON WITH THE STATE OF THE ART</h3>
<p>Finally, we compare our results with the state of the art in Table 7. In the classification task of ILSVRC-2014 challenge (Russakovsky et al., 2014), our “VGG” team secured the 2nd place with 7.3% test error using an ensemble of 7 models. After the submission, we decreased the error rate to 6.8% using an ensemble of 2 models.</p>
<p><img src="img/03-06.png" width="70%" style="display: block; margin: auto;" /></p>
<p>As can be seen from Table 7, our very deep ConvNets significantly outperform the previous generation of models, which achieved the best results in the ILSVRC-2012 and ILSVRC-2013 competitions. Our result is also competitive with respect to the classification task winner (GoogLeNet with 6.7% error) and substantially outperforms the ILSVRC-2013 winning submission Clarifai, which achieved 11.2% with outside training data and 11.7% without it. This is remarkable, considering that our best result is achieved by combining just two models – significantly less than used in most ILSVRC submissions. In terms of the single-net performance, our architecture achieves the best result (7.0% test error), outperforming a single GoogLeNet by 0.9%. Notably, we did not depart from the classical ConvNet architecture of LeCun et al. (1989), but improved it by substantially increasing the depth.</p>
<p>Table 7: <strong>Comparison with the state of the art in ILSVRC classification.</strong> Our method is denoted as “VGG”. Only the results obtained without outside training data are reported.</p>
<p><img src="img/03-07.png" width="70%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="conclusion" class="section level2" number="4.6">
<h2><span class="header-section-number">4.6</span> CONCLUSION</h2>
<p>In this work we evaluated very deep convolutional networks (up to 19 weight layers) for large-scale image classification. It was demonstrated that the representation depth is beneficial for the classification accuracy, and that state-of-the-art performance on the ImageNet challenge dataset can be achieved using a conventional ConvNet architecture (LeCun et al., 1989; Krizhevsky et al., 2012) with substantially increased depth. In the appendix, we also show that our models generalise well to a wide range of tasks and datasets, matching or outperforming more complex recognition pipelines built around less deep image representations. Our results yet again confirm the importance of depth in visual representations.</p>
<p><strong>ACKNOWLEDGEMENTS</strong>
This work was supported by ERC grant VisRec no. 228180. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the GPUs used for this research.</p>
<p><strong>REFERENCES</strong>
Bell, S., Upchurch, P., Snavely, N., and Bala, K. Material recognition in the wild with the materials in context database. CoRR, abs/1412.0623, 2014.</p>
<p>Chatfield, K., Simonyan, K., Vedaldi, A., and Zisserman, A. Return of the devil in the details: Delving deep into convolutional nets. In Proc. BMVC., 2014.</p>
<p>Cimpoi, M., Maji, S., and Vedaldi, A. Deep convolutional filter banks for texture recognition and segmentation. CoRR, abs/1411.6836, 2014.</p>
<p>Ciresan, D. C., Meier, U., Masci, J., Gambardella, L. M., and Schmidhuber, J. Flexible, high performance convolutional neural networks for image classification. In IJCAI, pp. 1237–1242, 2011.</p>
<p>Dean, J., Corrado, G., Monga, R., Chen, K., Devin, M., Mao, M., Ranzato, M., Senior, A., Tucker, P., Yang, K., Le, Q. V., and Ng, A. Y. Large scale distributed deep networks. In NIPS, pp. 1232–1240, 2012.</p>
<p>Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: A large-scale hierarchical image database. In Proc. CVPR, 2009.</p>
<p>Donahue, J., Jia, Y., Vinyals, O., Hoffman, J., Zhang, N., Tzeng, E., and Darrell, T. Decaf: A deep convolutional activation feature for generic visual recognition. CoRR, abs/1310.1531, 2013.</p>
<p>Everingham, M., Eslami, S. M. A., Van Gool, L., Williams, C., Winn, J., and Zisserman, A. The Pascal visual object classes challenge: A retrospective. IJCV, 111(1):98–136, 2015.</p>
<p>Fei-Fei, L., Fergus, R., and Perona, P. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In IEEE CVPR Workshop of Generative Model Based Vision, 2004.</p>
<p>Girshick, R. B., Donahue, J., Darrell, T., and Malik, J. Rich feature hierarchies for accurate object detection and semantic segmentation. CoRR, abs/1311.2524v5, 2014. Published in Proc. CVPR, 2014.</p>
<p>Gkioxari, G., Girshick, R., and Malik, J. Actions and attributes from wholes and parts. CoRR, abs/1412.2604, 2014.</p>
<p>Glorot, X. and Bengio, Y. Understanding the difficulty of training deep feedforward neural networks. In Proc. AISTATS, volume 9, pp. 249–256, 2010.</p>
<p>Goodfellow, I. J., Bulatov, Y., Ibarz, J., Arnoud, S., and Shet, V. Multi-digit number recognition from street view imagery using deep convolutional neural networks. In Proc. ICLR, 2014.</p>
<p>Griffin, G., Holub, A., and Perona, P. Caltech-256 object category dataset. Technical Report 7694, California Institute of Technology, 2007.</p>
<p>He, K., Zhang, X., Ren, S., and Sun, J. Spatial pyramid pooling in deep convolutional networks for visual recognition. CoRR, abs/1406.4729v2, 2014.</p>
<p>Hoai, M. Regularized max pooling for image categorization. In Proc. BMVC., 2014.</p>
<p>Howard, A. G. Some improvements on deep convolutional neural network based image classification. In Proc. ICLR, 2014.</p>
<p>Jia, Y. Caffe: An open source convolutional architecture for fast feature embedding. <a href="http://caffe.berkeleyvision.org/" class="uri">http://caffe.berkeleyvision.org/</a>, 2013.</p>
<p>Karpathy, A. and Fei-Fei, L. Deep visual-semantic alignments for generating image descriptions. CoRR, abs/1412.2306, 2014.</p>
<p>Kiros, R., Salakhutdinov, R., and Zemel, R. S. Unifying visual-semantic embeddings with multimodal neural language models. CoRR, abs/1411.2539, 2014.</p>
<p>Krizhevsky, A. One weird trick for parallelizing convolutional neural networks. CoRR, abs/1404.5997, 2014.</p>
<p>Krizhevsky, A., Sutskever, I., and Hinton, G. E. ImageNet classification with deep convolutional neural networks. In NIPS, pp. 1106–1114, 2012.</p>
<p>LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard, W., and Jackel, L. D. Backpropagation applied to handwritten zip code recognition. Neural Computation, 1(4):541–551, 1989.</p>
<p>Lin, M., Chen, Q., and Yan, S. Network in network. In Proc. ICLR, 2014.</p>
<p>Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks for semantic segmentation. CoRR, abs/1411.4038, 2014.</p>
<p>Oquab, M., Bottou, L., Laptev, I., and Sivic, J. Learning and Transferring Mid-Level Image Representations using Convolutional Neural Networks. In Proc. CVPR, 2014.</p>
<p>Perronnin, F., Sa ́nchez, J., and Mensink, T. Improving the Fisher kernel for large-scale image classification. In Proc. ECCV, 2010.</p>
<p>Razavian, A., Azizpour, H., Sullivan, J., and Carlsson, S. CNN Features off-the-shelf: an Astounding Baseline for Recognition. CoRR, abs/1403.6382, 2014.</p>
<p>Russakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., Huang, Z., Karpathy, A., Khosla, A., Bernstein, M., Berg, A. C., and Fei-Fei, L. ImageNet large scale visual recognition challenge. CoRR, abs/1409.0575, 2014.</p>
<p>Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., and LeCun, Y. OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks. In Proc. ICLR, 2014.</p>
<p>Simonyan, K. and Zisserman, A. Two-stream convolutional networks for action recognition in videos. CoRR, abs/1406.2199, 2014. Published in Proc. NIPS, 2014.</p>
<p>Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V., and Rabinovich, A. Going deeper with convolutions. CoRR, abs/1409.4842, 2014.</p>
<p>Wei, Y., Xia, W., Huang, J., Ni, B., Dong, J., Zhao, Y., and Yan, S. CNN: Single-label to multi-label. CoRR, abs/1406.5726, 2014.</p>
<p>Zeiler, M. D. and Fergus, R. Visualizing and understanding convolutional networks. CoRR, abs/1311.2901, 2013. Published in Proc. ECCV, 2014.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="经典深度学习网络模型.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/iotctech/deeplearning/edit/master/03-VGG.Rmd",
"text": "编辑"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["Deep-Learning-Book.pdf", "Deep-Learning-Book.epub"],
"toc": {
"collapse": "section"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
